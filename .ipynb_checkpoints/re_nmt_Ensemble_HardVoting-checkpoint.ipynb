{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EoRVv9GoMn0"
   },
   "source": [
    "# **Attention Ensemble  - Hard Voting**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI5Vl6b8ohHm"
   },
   "source": [
    "### **구글 드라이브 Mount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "ktx3xvhwc2rL"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V42Oa1cP8eh"
   },
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "6C5I92usDlWa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TwhXqshQD_d"
   },
   "source": [
    "### **데이터 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "2LGie5tSDrLl"
   },
   "outputs": [],
   "source": [
    "# path_to_file = '/content/drive/MyDrive/Colab Notebooks/spa-eng/spa.txt'\n",
    "# path_to_file_esb = '/content/drive/MyDrive/Colab Notebooks/spa-eng/spa_for_esb.txt'\n",
    "path_to_file = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/spa-eng/spa.txt'\n",
    "path_to_file_esb = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/spa-eng/spa_for_esb.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXLKdsBsE73t"
   },
   "source": [
    "### **데이터 랜덤 셔플**\n",
    "\n",
    "\n",
    "*   [영어, 스페인어] 쌍 shuffle file 새로 저장\n",
    "*   번역 테스트 시 주석 처리\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "kcT0PKIZE7Zx"
   },
   "outputs": [],
   "source": [
    "# lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "# random.shuffle(lines)\n",
    "# f = open(path_to_file_esb, 'w')\n",
    "# for i in lines:\n",
    "#     data = i + '\\n'\n",
    "#     f.write(data)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpevBqz3QLgl"
   },
   "source": [
    "### **데이터(문장) 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "NvfW9HTkENRY"
   },
   "outputs": [],
   "source": [
    "# 유니코드 파일을 아스키 코드 파일로 변환합니다.\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성합니다.\n",
    "  # 예시: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체합니다.\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서\n",
    "  # 문장에 start와 end 토큰을 추가합니다.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCvca5_mQZvK"
   },
   "source": [
    "### **Dataset 생성**\n",
    "1. 문장에 있는 억양을 제거합니다.\n",
    "2. 불필요한 문자를 제거하여 문장을 정리합니다.\n",
    "3. 다음과 같은 형식으로 문장의 쌍을 반환합니다: [영어, 스페인어]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "3VXgr4_2EUgt"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImjigtL7QrJD"
   },
   "source": [
    "### **Language 가 들어오면 공백 단위로 토큰화**\n",
    "- fit_on_texts(): 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
    "- texts_to_sequences: 텍스트 안의 단어들을 숫자 시퀀스로 출력\n",
    "- pad_sequcences(tensor, padding='post') : 서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들어주기 위해 패딩을 사용\n",
    "  - padding = 'post' : [[ 0  0  0  5  3  2  4], [ 0  0  0  5  3  2  7],...,]\n",
    "  - padding = 'pre' : 뒤 부터 패딩이 채워짐\n",
    "  - 가장 긴 sequence 의 길이 만큼\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "u9VUFvjLEdER"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXvBdPFoU1i-"
   },
   "source": [
    "### **전처리된 타겟 문장과 입력 문장 쌍을 생성**\n",
    "- input_tensor : input 문장의 패딩 처리된 숫자 시퀀스\n",
    "- inp_lang_tokenizer : input 문장을 공백 단위로 토큰화, 문자 -> 리스트 변환\n",
    "- target_tensor, targ_lang_tokenizer : 위와 비슷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Rc4Rbx8JEjQZ"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  \n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NykrAhCiX_nn"
   },
   "source": [
    "### **언어 데이터셋 크기 제한**\n",
    "- 언어 데이터셋을 아래의 크기로 제한하여 훈련과 검증을 수행\n",
    "- inp_lang, targ_lang : 인풋,타겟 문장의 문자 -> 리스트 변환 결과\n",
    "- max_length_targ, max_length_inp : 인풋, 타겟 문장의 '패딩된' 숫자 시퀀스 길이 -> 타겟 텐서와 입력 텐서의 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ioibz-1bEkx-"
   },
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file_esb, num_examples)\n",
    "# input_tensor2, target_tensor2, inp_lang2, targ_lang2 = load_dataset(path_to_file, 1, num_examples)\n",
    "# input_tensor3, target_tensor3, inp_lang3, targ_lang3 = load_dataset(path_to_file, 2, num_examples)\n",
    "\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "# max_length_targ2, max_length_inp2 = target_tensor2.shape[1], input_tensor2.shape[1]\n",
    "# max_length_targ3, max_length_inp3 = target_tensor3.shape[1], input_tensor3.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQRUdPQDMu8m"
   },
   "source": [
    "### **데이터셋 (테스트 & 검증) 분리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7I7aQZrMuKM",
    "outputId": "37a64684-bb7d-4c61-da80-7ddc526232c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# 훈련 집합과 검증 집합을 80대 20으로 분리합니다.\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 훈련 집합과 검증 집합의 데이터 크기를 출력합니다.\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XAy3EiWZz3b"
   },
   "source": [
    "### 인덱스 -> 해당 word 로\n",
    "\n",
    "```\n",
    "Input Language; index to word mapping\n",
    "1 ----> <start>\n",
    "93 ----> tomas\n",
    "27 ----> le\n",
    "1063 ----> escribio\n",
    "7 ----> a\n",
    "120 ----> maria\n",
    "3 ----> .\n",
    "2 ----> <end>\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Target Language; index to word mapping\n",
    "1 ----> <start>\n",
    "8 ----> tom\n",
    "695 ----> wrote\n",
    "6 ----> to\n",
    "31 ----> mary\n",
    "3 ----> .\n",
    "2 ----> <end>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "XwDUP11tErzK"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrwIipPCbcBB"
   },
   "source": [
    "### **Buffer, Batch, epoch, embedding dimension, units 설정**\n",
    "- Tokenizer 의 word_index 속성 : 속성은 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환\n",
    "- 따라서 vocab_inp_size, vocab_inp_size : 인풋, 타겟의 단어-숫자 딕셔너리 최대 길이 + 1 (?)\n",
    "- dataset.batch(BATCH_SIZE, drop_remainder = True) : 배치사이즈 만큼 분할 후 남은 데이터를 drop 할 것인지 여부\n",
    "- shuffle : 데이터셋 적절히 섞어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "hU7tb2GYEvZX"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "# 훈련 집합에서만 shuffle, batch\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MitMbnt45EM",
    "outputId": "97b665ac-8559-459e-a7c1-455e4eb8d655"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 53]), TensorShape([64, 51]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXVK8Dv4cOi7"
   },
   "source": [
    "### **Encoder**\n",
    "\n",
    "\n",
    "1.   초기화 : vocab_size(단어의 크기), embedding_dim(임베딩 차원 수), enc_units(인코더의 히든 사이즈), batch_sz(배치 사이즈)\n",
    "  - embedding_dim : 단어 -> 임베딩 벡터로 하기 위한 차원 수\n",
    "2.  call : gru 에 들어가 output, state 출력\n",
    "3.  initialize_hidden_state : 맨 처음 gru에 들어가기 위한 더미 입력 값\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "AvYcCHoFE7UY"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBEgB8_Lf1Wx"
   },
   "source": [
    "### **Encoder 객체 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "LsoTBEKRFgkQ"
   },
   "outputs": [],
   "source": [
    "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXbulBGnesz7"
   },
   "source": [
    "### **Attention**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "2T7avh_BFVZN"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # 쿼리 은닉 상태(query hidden state)는 (batch_size, hidden size)쌍으로 이루어져 있습니다.\n",
    "    # query_with_time_axis은 (batch_size, 1, hidden size)쌍으로 이루어져 있습니다.\n",
    "    # values는 (batch_size, max_len, hidden size)쌍으로 이루어져 있습니다.\n",
    "    # 스코어(score)계산을 위해 덧셈을 수행하고자 시간 축을 확장하여 아래의 과정을 수행합니다.\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다.\n",
    "    # score를 self.V에 적용하기 때문에 마지막 축에 1을 얻습니다.\n",
    "    # self.V에 적용하기 전에 텐서는 (batch_size, max_length, units)쌍으로 이루어져 있습니다.\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다. \n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # 덧셈이후 컨텍스트 벡터(context_vector)는 (batch_size, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOyJPT-ze23p"
   },
   "source": [
    "### **Decoder**\n",
    "\n",
    "\n",
    "1.   초기화 : vocab_size(단어의 크기), embedding_dim(임베딩 차원 수), enc_units(인코더의 히든 사이즈), batch_sz(배치 사이즈)\n",
    "2.   encoder 와의 차이점 : 마지막 fully_connected_layer(tf.keras.layers.Dense) 추가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "weUzeqB1FaVk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # 어텐션을 사용합니다.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output는 (batch_size, max_length, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # 임베딩층을 통과한 후 x는 (batch_size, 1, embedding_dim)쌍으로 이루어져 있습니다.\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # 컨텍스트 벡터와 임베딩 결과를 결합한 이후 x의 형태는 (batch_size, 1, embedding_dim + hidden_size)쌍으로 이루어져 있습니다.\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # 위에서 결합된 벡터를 GRU에 전달합니다.\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output은 (batch_size * 1, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output은 (batch_size, vocab)쌍으로 이루어져 있습니다.\n",
    "    x = self.fc(output)\n",
    "\n",
    "    # return x, state, attention_weights\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG7bJQ8sf61-"
   },
   "source": [
    "### **Decoder 객체 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "NoeGR2CsFk8E"
   },
   "outputs": [],
   "source": [
    "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmQNy2L3Qn-M"
   },
   "source": [
    "### **NMT Model 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "cWwIOiaEQnr9"
   },
   "outputs": [],
   "source": [
    "class NMT_Model():\n",
    "  def __init__(self):\n",
    "    super(NMT_Model, self).__init__()\n",
    "    self.encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "    self.decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoywC60GQDIR"
   },
   "source": [
    "### **Ensemble Model 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "CkYKjqyoQCq5"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "num_models = 3\n",
    "for m in range(num_models):\n",
    "  m = NMT_Model()\n",
    "  models.append(m)\n",
    "\n",
    "# # 각 모델 호출하려면 \n",
    "# for model in models:\n",
    "#   print(model)\n",
    "# for m in range(num_models):\n",
    "#     models.append(NMT_Model())\n",
    "# print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "PjW32k9o4HnQ"
   },
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#   sample_hidden = model.encoder.initialize_hidden_state()\n",
    "#   sample_output, sample_hidden = model.encoder(example_input_batch, sample_hidden)\n",
    "#   print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "#   print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "w0hr-h536Azk"
   },
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#   sample_decoder_output, _ = model.decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "#                                         sample_hidden, sample_output)\n",
    "#   print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrfLbnT6gAXp"
   },
   "source": [
    "### **Loss Function & Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "QFs5xbUXFmPH"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43_Spvj-gOsG"
   },
   "source": [
    "### **Chekcpoint**\n",
    "- 여기서 학습한 매개변수를 저장, optimizer/encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Z7GWRTtRFoGz"
   },
   "outputs": [],
   "source": [
    "# checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/training_checkpoints_esb'\n",
    "# checkpoint_dir_test = '/content/drive/MyDrive/Colab Notebooks/training_checkpoints_esb2'\n",
    "checkpoint_dir = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/training_checkpoints_esb'\n",
    "checkpoint_dir_test = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/training_checkpoints_esb2'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoints = []\n",
    "\n",
    "for m in range(num_models):\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=models[m].encoder,\n",
    "                                 decoder=models[m].decoder)\n",
    "  checkpoints.append(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dtBOTNp93pQ",
    "outputId": "28f8d7e7-3fff-487f-a253-de0f62559f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.training.tracking.util.Checkpoint object at 0x7fe89501fb38>, <tensorflow.python.training.tracking.util.Checkpoint object at 0x7fe8aae83470>, <tensorflow.python.training.tracking.util.Checkpoint object at 0x7fe895104630>]\n"
     ]
    }
   ],
   "source": [
    "print(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPqtJ9YsWqG6"
   },
   "source": [
    "### **Train_step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "W4M_cQT0kp5S"
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(model, inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = model.encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # enc_output를 디코더에 전달합니다.\n",
    "      predictions, dec_hidden = model.decoder(dec_input, dec_hidden, enc_output)\n",
    "      # print('predictions', predictions.shape)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # 교사 강요(teacher forcing)를 사용합니다. -> 훈련에서는 실제 값을 이용\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = model.encoder.trainable_variables + model.decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unWNByYCIae0"
   },
   "source": [
    "### **학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_D8iS6wdus77",
    "outputId": "6ed12ed2-3e47-48fe-8491-937b7daa869b"
   },
   "outputs": [],
   "source": [
    "# EPOCHS = 10\n",
    "# model_loss = {0 : 0, 1 : 0, 2 : 0}\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   start = time.time()\n",
    "\n",
    "#   total_loss_0 = 0\n",
    "#   total_loss_1 = 0\n",
    "#   total_loss_2 = 0\n",
    "#   enc_hidden_0 = models[0].encoder.initialize_hidden_state()\n",
    "#   enc_hidden_1 = models[1].encoder.initialize_hidden_state()\n",
    "#   enc_hidden_2 = models[2].encoder.initialize_hidden_state()\n",
    "\n",
    "#   # m1_train_step = train_step()\n",
    "#   # m2_train_step = train_step()\n",
    "#   for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "#       batch_loss_0 = train_step(models[0], inp, targ, enc_hidden_0)\n",
    "#       batch_loss_1 = train_step(models[1], inp, targ, enc_hidden_1)\n",
    "#       batch_loss_2 = train_step(models[0], inp, targ, enc_hidden_2)\n",
    "\n",
    "#       total_loss_0 += batch_loss_0\n",
    "#       total_loss_1 += batch_loss_1\n",
    "#       total_loss_2 += batch_loss_2\n",
    "\n",
    "#       if batch % 100 == 0:\n",
    "#         print('Model {} Epoch {} Batch {} Loss {:.4f}'.format(models[0], epoch + 1,\n",
    "#                                                     batch,\n",
    "#                                                     batch_loss_0.numpy()))\n",
    "#         print('Model {} Epoch {} Batch {} Loss {:.4f}'.format(models[1], epoch + 1,\n",
    "#                                                     batch,\n",
    "#                                                     batch_loss_1.numpy()))\n",
    "#         print('Model {} Epoch {} Batch {} Loss {:.4f}'.format(models[2], epoch + 1,\n",
    "#                                                     batch,\n",
    "#                                                     batch_loss_2.numpy()))\n",
    "#   # 각 모델의 최종 loss 를 저장\n",
    "#   model_loss[0] = total_loss_0\n",
    "#   model_loss[1] = total_loss_1\n",
    "#   model_loss[2] = total_loss_2\n",
    "\n",
    "#   # 에포크가 2번 실행될때마다 모델 저장 (모델 별 체크포인트)\n",
    "#   if (epoch + 1) % 2 == 0:\n",
    "#     for idx, checkpoint in enumerate(checkpoints):\n",
    "#       checkpoint.save(file_prefix=checkpoint_prefix+'-{}'.format(idx))\n",
    "\n",
    "#   print('Model 1 : Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       model_loss[0] / steps_per_epoch))\n",
    "#   print('Model 2 : Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       model_loss[1] / steps_per_epoch))\n",
    "#   print('Model 3 : Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       model_loss[2] / steps_per_epoch))\n",
    "#   print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "qhjmV94vIZzU"
   },
   "outputs": [],
   "source": [
    "# EPOCHS = 1\n",
    "# model_loss = {0 : 0, 1 : 0, 2 : 0}\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   start = time.time()\n",
    "  \n",
    "#   for i, model in enumerate(models):\n",
    "#     # print('model', model)\n",
    "#     # train_encoder = model.encoder\n",
    "#     # train_decoder = model.decoder\n",
    "#     # print(train_encoder)\n",
    "#     # print(train_decoder)\n",
    "#     total_loss = 0\n",
    "#     enc_hidden = model.encoder.initialize_hidden_state()\n",
    "\n",
    "#     for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "#       batch_loss = train_step(model, inp, targ, enc_hidden)\n",
    "\n",
    "#       total_loss += batch_loss\n",
    "\n",
    "#       if batch % 100 == 0:\n",
    "#         print('Model {} Epoch {} Batch {} Loss {:.4f}'.format(model, epoch + 1,\n",
    "#                                                     batch,\n",
    "#                                                     batch_loss.numpy()))\n",
    "#     # 각 모델의 최종 loss 를 저장\n",
    "#     model_loss[i] = total_loss  \n",
    "  \n",
    "#     # 에포크가 2번 실행될때마다 모델 저장 (모델 별 체크포인트)\n",
    "#   if (epoch + 1) % 2 == 0:\n",
    "#     for idx, checkpoint in enumerate(checkpoints):\n",
    "#       checkpoint.save(file_prefix=checkpoint_prefix+'-{}'.format(idx))\n",
    "\n",
    "#   print('Model 1 : Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       model_loss[0] / steps_per_epoch))\n",
    "#   print('Model 2 : Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       model_loss[1] / steps_per_epoch))\n",
    "#   # print('Model 3 : Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#   #                                     model_loss[2] / steps_per_epoch))\n",
    "#   print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfZWTXkHgZua"
   },
   "source": [
    "### **문장 번역(스페인 -> 영어)** \n",
    "\n",
    "*   tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen='', padding='post') : 일정한 길이(maxlen)로 맞춰준다. (패딩은 뒤에서)\n",
    "*   \n",
    "\n",
    "  ```\n",
    "  inp_lang.word_index :  {'<start>': 1, '<end>': 2, '.': 3, 'tom': 4, '?': 5...}\n",
    "  ```\n",
    "\n",
    "* tf.expand_dims: 차원을 늘려준다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "h-wAUSiGFujZ"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  # 어텐션 그래프\n",
    "  # attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  # print('model1 : ', models[0])\n",
    "  # print('model2 : ', models[1])\n",
    "  # print('model3 : ', models[2])\n",
    "\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 문장, input 딕셔너리 출력 \n",
    "  print ('sentence:', sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs2 = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs2 = tf.keras.preprocessing.sequence.pad_sequences([inputs2],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs3 = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs3 = tf.keras.preprocessing.sequence.pad_sequences([inputs3],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inputs2 = tf.convert_to_tensor(inputs2)\n",
    "  inputs3 = tf.convert_to_tensor(inputs3)\n",
    "\n",
    "\n",
    "  result1 = ''\n",
    "  result2 = ''\n",
    "  result3 = ''\n",
    "  voting_result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  hidden2 = [tf.zeros((1, units))]\n",
    "  hidden3 = [tf.zeros((1, units))]\n",
    "\n",
    "  # Encoder 의 hidden 을 Decoder 의 hidden 으로 받는다.\n",
    "  enc_out, enc_hidden = models[0].encoder(inputs, hidden)\n",
    "\n",
    "  enc_out2, enc_hidden2 = models[1].encoder(inputs2, hidden2)\n",
    "\n",
    "  enc_out3, enc_hidden3 = models[2].encoder(inputs3, hidden3)\n",
    "\n",
    "\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_hidden2 = enc_hidden2\n",
    "  dec_hidden3 = enc_hidden3\n",
    "\n",
    "\n",
    "  # Decoder 의 시작인 '<start>' \n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "  dec_input2 = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "  dec_input3 = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  # Target 의 최대 길이 만큼 출력\n",
    "  for t in range(max_length_inp):\n",
    "    predictions, dec_hidden = models[0].decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "    predictions2, dec_hidden2 = models[1].decoder(dec_input2,\n",
    "                                                         dec_hidden2,\n",
    "                                                         enc_out2)\n",
    "    predictions3, dec_hidden3 = models[2].decoder(dec_input3,\n",
    "                                                         dec_hidden3,\n",
    "                                                         enc_out3)\n",
    "    \n",
    "    predicted_id = tf.argmax(predictions[0]).numpy() \n",
    "    predicted_id2 = tf.argmax(predictions2[0]).numpy() \n",
    "    predicted_id3 = tf.argmax(predictions3[0]).numpy() \n",
    "\n",
    "    voting = {}\n",
    "    if predicted_id not in voting:\n",
    "      voting[predicted_id] = 1\n",
    "    else :\n",
    "      voting[predicted_id] += 1\n",
    "    \n",
    "    if predicted_id2 not in voting:\n",
    "      voting[predicted_id2] = 1\n",
    "    else :\n",
    "      voting[predicted_id2] += 1\n",
    "    \n",
    "    if predicted_id3 not in voting:\n",
    "      voting[predicted_id3] = 1\n",
    "    else :\n",
    "      voting[predicted_id3] += 1\n",
    "    print(voting)\n",
    "    \n",
    "    print(max(voting,key=voting.get)) # di.get 이용\n",
    "\n",
    "    voting_id = max(voting,key=voting.get)\n",
    "\n",
    "  # for t in range(max_length_targ):\n",
    "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "  #                                                        dec_hidden,\n",
    "  #                                                        enc_out)\n",
    "\n",
    "    # 나중에 어텐션 가중치를 시각화하기 위해 어텐션 가중치를 저장합니다.\n",
    "    # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    # attention_plot[t] = attention_weights.numpy()\n",
    "    \n",
    "    result1 += targ_lang.index_word[predicted_id] + ' '\n",
    "    result2 += targ_lang.index_word[predicted_id2] + ' '\n",
    "    result3 += targ_lang.index_word[predicted_id3] + ' '\n",
    "    voting_result += targ_lang.index_word[voting_id] + ' '\n",
    "    \n",
    "#     if (targ_lang.index_word[predicted_id2] == '<end>') or (targ_lang.index_word[predicted_id2] == '<end>') or (targ_lang.index_word[predicted_id] == '<end>'):\n",
    "#       return result1, result2, result3, voting_result, sentence\n",
    "\n",
    "    if targ_lang.index_word[voting_id] == '<end>':\n",
    "      return result1, result2, result3, voting_result, sentence\n",
    "\n",
    "    # 예측된 ID를 모델에 다시 피드합니다. (voting_id)\n",
    "    dec_input = tf.expand_dims([voting_id], 0)\n",
    "    dec_input2 = tf.expand_dims([voting_id], 0)\n",
    "    dec_input3 = tf.expand_dims([voting_id], 0)\n",
    "\n",
    "  # return result, sentence, attention_plot\n",
    "  return result1, result2, result3, voting_result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "X1uJo_rwvQxJ"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  # result, sentence = evaluate(sentence)\n",
    "  result1, result2, result3, voting_result, sentence = evaluate(sentence)\n",
    "  # result1, result2, sentence = evaluate(sentence)\n",
    "  \n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Model 1 의 Predicted translation: {}'.format(result1))\n",
    "  print('Model 2 의 Predicted translation: {}'.format(result2))\n",
    "  print('Model 3 의 Predicted translation: {}'.format(result3))\n",
    "  print('HardVoting 의 Predicted translation: {}'.format(voting_result))\n",
    "\n",
    "  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "#   plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R85_0yJYCxvw"
   },
   "source": [
    "### **Checkpoint 복원**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_dir_m1 = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/training_checkpoints_esb'\n",
    "ckp_dir_m2 = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/training_checkpoints_esb 2'\n",
    "ckp_dir_m3 = '/Users/ahjeong_park/Study/Attention-Ensemble-Translation/training_checkpoints_esb 3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "4lQepsNS0L24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe8950e6c88>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_dir내에 있는 최근 체크포인트(checkpoint)를 복원\n",
    "checkpoints[0].restore(tf.train.latest_checkpoint(ckp_dir_m1))\n",
    "checkpoints[1].restore(tf.train.latest_checkpoint(ckp_dir_m2))\n",
    "checkpoints[2].restore(tf.train.latest_checkpoint(ckp_dir_m3))\n",
    "\n",
    "### 이 코드로 했을 때 학습 바로 돌렸을 때와 같은 결과가 나왔음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZYGST44C5Ym"
   },
   "source": [
    "### **번역 시작**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "cUDjZTCKvZA5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> hace mucho frio aqui . <end>\n",
      "{14: 3}\n",
      "14\n",
      "{15: 2, 11: 1}\n",
      "15\n",
      "{58: 2, 9: 1}\n",
      "58\n",
      "{290: 3}\n",
      "290\n",
      "{62: 1, 3: 1, 47: 1}\n",
      "62\n",
      "{3: 3}\n",
      "3\n",
      "{2: 3}\n",
      "2\n",
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Model 1 의 Predicted translation: it s very cold here . <end> \n",
      "Model 2 의 Predicted translation: it is a cold . . <end> \n",
      "Model 3 의 Predicted translation: it s very cold there . <end> \n",
      "HardVoting 의 Predicted translation: it s very cold here . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')  # it s very cold here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "76Goju9Rvch8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> esta es mi vida . <end>\n",
      "{23: 3}\n",
      "23\n",
      "{11: 3}\n",
      "11\n",
      "{25: 3}\n",
      "25\n",
      "{188: 3}\n",
      "188\n",
      "{3: 3}\n",
      "3\n",
      "{2: 3}\n",
      "2\n",
      "Input: <start> esta es mi vida . <end>\n",
      "Model 1 의 Predicted translation: this is my life . <end> \n",
      "Model 2 의 Predicted translation: this is my life . <end> \n",
      "Model 3 의 Predicted translation: this is my life . <end> \n",
      "HardVoting 의 Predicted translation: this is my life . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')  # this is my life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "dvrhSTomviTt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> ¿ todavia estan en casa ? <end>\n",
      "{34: 3}\n",
      "34\n",
      "{7: 3}\n",
      "7\n",
      "{153: 3}\n",
      "153\n",
      "{43: 1, 114: 2}\n",
      "114\n",
      "{10: 3}\n",
      "10\n",
      "{2: 3}\n",
      "2\n",
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Model 1 의 Predicted translation: are you still at ? <end> \n",
      "Model 2 의 Predicted translation: are you still home ? <end> \n",
      "Model 3 의 Predicted translation: are you still home ? <end> \n",
      "HardVoting 의 Predicted translation: are you still home ? <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')  # Are you still at home?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "OTXXZwkdvg_Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> trata de averiguarlo . <end>\n",
      "{277: 3}\n",
      "277\n",
      "{6: 3}\n",
      "6\n",
      "{864: 2, 191: 1}\n",
      "864\n",
      "{14: 3}\n",
      "14\n",
      "{69: 3}\n",
      "69\n",
      "{3: 3}\n",
      "3\n",
      "{2: 3}\n",
      "2\n",
      "Input: <start> trata de averiguarlo . <end>\n",
      "Model 1 의 Predicted translation: try to figure it out . <end> \n",
      "Model 2 의 Predicted translation: try to figure it out . <end> \n",
      "Model 3 의 Predicted translation: try to find it out . <end> \n",
      "HardVoting 의 Predicted translation: try to figure it out . <end> \n"
     ]
    }
   ],
   "source": [
    "# 잘못된 번역\n",
    "translate(u'trata de averiguarlo.')   # try to find out / try to figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "pwmBqLU8cvE6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> te quiero <end>\n",
      "{4: 3}\n",
      "4\n",
      "{146: 2, 38: 1}\n",
      "146\n",
      "{7: 3}\n",
      "7\n",
      "{3: 2, 54: 1}\n",
      "3\n",
      "{2: 2, 7: 1}\n",
      "2\n",
      "Input: <start> te quiero <end>\n",
      "Model 1 의 Predicted translation: i love you . <end> \n",
      "Model 2 의 Predicted translation: i want you how you \n",
      "Model 3 의 Predicted translation: i love you . <end> \n",
      "HardVoting 의 Predicted translation: i love you . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Te quiero')   # I love you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "UN-fn_770P3m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> esta es mi vida . <end>\n",
      "{23: 3}\n",
      "23\n",
      "{11: 3}\n",
      "11\n",
      "{25: 3}\n",
      "25\n",
      "{188: 3}\n",
      "188\n",
      "{3: 3}\n",
      "3\n",
      "{2: 3}\n",
      "2\n",
      "Input: <start> esta es mi vida . <end>\n",
      "Model 1 의 Predicted translation: this is my life . <end> \n",
      "Model 2 의 Predicted translation: this is my life . <end> \n",
      "Model 3 의 Predicted translation: this is my life . <end> \n",
      "HardVoting 의 Predicted translation: this is my life . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')  # this is my life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 긴 문장 번역 Test ###\n",
    "\n",
    "Le preguntó qué estaba pasando, pero ella no dijo nada.\n",
    "\n",
    "He asked her what was going on, but she didn't say anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: <start> le pregunto que estaba pasando , pero ella no dijo nada . <end>\n",
      "{13: 2, 28: 1}\n",
      "13\n",
      "{181: 3}\n",
      "181\n",
      "{46: 3}\n",
      "46\n",
      "{30: 3}\n",
      "30\n",
      "{20: 2, 4: 1}\n",
      "20\n",
      "{946: 2, 122: 1}\n",
      "946\n",
      "{19: 3}\n",
      "19\n",
      "{95: 3}\n",
      "95\n",
      "{28: 3}\n",
      "28\n",
      "{66: 3}\n",
      "66\n",
      "{12: 3}\n",
      "12\n",
      "{44: 1, 136: 2}\n",
      "136\n",
      "{130: 3}\n",
      "130\n",
      "{3: 3}\n",
      "3\n",
      "{2: 3}\n",
      "2\n",
      "Input: <start> le pregunto que estaba pasando , pero ella no dijo nada . <end>\n",
      "Model 1 의 Predicted translation: he asked him what was happening , but she didn t know anything . <end> \n",
      "Model 2 의 Predicted translation: he asked him what was happening , but she didn t say anything . <end> \n",
      "Model 3 의 Predicted translation: she asked him what i said , but she didn t say anything . <end> \n",
      "HardVoting 의 Predicted translation: he asked him what was happening , but she didn t say anything . <end> \n"
     ]
    }
   ],
   "source": [
    "long_test = u'Le preguntó qué estaba pasando, pero ella no dijo nada.'\n",
    "translate(long_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 긴 문장 번역 Test ###\n",
    "\n",
    "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
    "\n",
    "If you want to sound like a native speaker, you must be willing toq practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_test2 = u'Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.'\n",
    "\n",
    "translate(long_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finalmente estaba completamente libre del dolor en su brazo y caminó con alegría hacia el árbol de la mano.\n",
    "\n",
    "She was finally completely free from the pain in her arm and walked with joy to the hand tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_test3 = u'Finalmente estaba completamente libre del dolor en su brazo y caminó con alegría hacia el árbol de la mano.'\n",
    "\n",
    "translate(long_test3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "re-nmt_Ensemble_HardVoting",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
