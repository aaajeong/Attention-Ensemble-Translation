{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"nmt_with_attetion_Ensemble_SoftVoting.ipynb","provenance":[{"file_id":"1YDxomkNK-KSFt-JlFsQpz0_xiqUwihOn","timestamp":1620894670470}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 랜덤 x \n","# [0:30000], [30000:60000], [60000:90000]\n","# 총 3개 모델을 Ensemble\n","# Soft Voting"]},{"cell_type":"code","metadata":{"id":"ktx3xvhwc2rL","executionInfo":{"status":"ok","timestamp":1620897684265,"user_tz":-540,"elapsed":1271,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"6C5I92usDlWa","executionInfo":{"status":"ok","timestamp":1620897687646,"user_tz":-540,"elapsed":4648,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["import tensorflow as tf\n","from keras.models import Model\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split \n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LGie5tSDrLl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620897687650,"user_tz":-540,"elapsed":4646,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"eecc9c6e-8abb-4e34-953a-84b39410cc3d"},"source":["# 데이터 로드\n","path_to_file = '/content/drive/MyDrive/Colab Notebooks/spa-eng/spa.txt'\n","print(path_to_file)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/spa-eng/spa.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NvfW9HTkENRY","executionInfo":{"status":"ok","timestamp":1620897687650,"user_tz":-540,"elapsed":4645,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# 유니코드 파일을 아스키 코드 파일로 변환합니다.\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","\n","  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성합니다.\n","  # 예시: \"he is a boy.\" => \"he is a boy .\"\n","  # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체합니다.\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","  w = w.strip()\n","\n","  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서\n","  # 문장에 start와 end 토큰을 추가합니다.\n","  w = '<start> ' + w + ' <end>'\n","  return w"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VXgr4_2EUgt","executionInfo":{"status":"ok","timestamp":1620897687651,"user_tz":-540,"elapsed":4643,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# 1. 문장에 있는 억양을 제거합니다.\n","# 2. 불필요한 문자를 제거하여 문장을 정리합니다.\n","# 3. 다음과 같은 형식으로 문장의 쌍을 반환합니다: [영어, 스페인어]\n","def create_dataset(path, num_examples, range):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[(num_examples*range):(num_examples*(range+1))]]\n","\n","  return zip(*word_pairs)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9VUFvjLEdER","executionInfo":{"status":"ok","timestamp":1620897687652,"user_tz":-540,"elapsed":4642,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# language 가 들어오면 공백 단위로 토큰화\n","def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","  lang_tokenizer.fit_on_texts(lang)\n","\n","  # texts_to_sequqences : 텍스트 안의 단어들을 숫자 시퀀스로 출력\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","    \n","  # 서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들어주기 위해 패딩을 사용\n","  # post : 뒤에 패딩이 채워짐 (앞에서부터는 'pre')\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","  return tensor, lang_tokenizer"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rc4Rbx8JEjQZ","executionInfo":{"status":"ok","timestamp":1620897687652,"user_tz":-540,"elapsed":4640,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["def load_dataset(path, range, num_examples=None):\n","  # 전처리된 타겟 문장과 입력 문장 쌍을 생성합니다.\n","  targ_lang, inp_lang = create_dataset(path, num_examples, range)\n","\n","  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ioibz-1bEkx-","executionInfo":{"status":"ok","timestamp":1620897697806,"user_tz":-540,"elapsed":14792,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# 언어 데이터셋을 아래의 크기로 제한하여 훈련과 검증을 수행합니다.\n","num_examples = 30000\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, 0, num_examples)\n","input_tensor2, target_tensor2, inp_lang2, targ_lang2 = load_dataset(path_to_file, 1, num_examples)\n","input_tensor3, target_tensor3, inp_lang3, targ_lang3 = load_dataset(path_to_file, 2, num_examples)\n","\n","\n","# 타겟 텐서와 입력 텐서의 최대 길이를 계산합니다.\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n","max_length_targ2, max_length_inp2 = target_tensor2.shape[1], input_tensor2.shape[1]\n","max_length_targ3, max_length_inp3 = target_tensor3.shape[1], input_tensor3.shape[1]\n","\n","# print(max_length_targ, max_length_inp)\n","# print(max_length_targ2, max_length_inp2)\n","# print(max_length_targ3, max_length_inp3)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwDUP11tErzK","executionInfo":{"status":"ok","timestamp":1620897697809,"user_tz":-540,"elapsed":14794,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# 인덱스 -> 해당 word 로.\n","def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"hU7tb2GYEvZX","executionInfo":{"status":"ok","timestamp":1620897697810,"user_tz":-540,"elapsed":14793,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","# steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","vocab_inp_size2 = len(inp_lang2.word_index)+1\n","vocab_tar_size2 = len(targ_lang2.word_index)+1\n","\n","vocab_inp_size3 = len(inp_lang3.word_index)+1\n","vocab_tar_size3 = len(targ_lang3.word_index)+1"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvYcCHoFE7UY","executionInfo":{"status":"ok","timestamp":1620897697810,"user_tz":-540,"elapsed":14791,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsoTBEKRFgkQ","executionInfo":{"status":"ok","timestamp":1620897702575,"user_tz":-540,"elapsed":19555,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","encoder2 = Encoder(vocab_inp_size2, embedding_dim, units, BATCH_SIZE)\n","encoder3 = Encoder(vocab_inp_size3, embedding_dim, units, BATCH_SIZE)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"2T7avh_BFVZN","executionInfo":{"status":"ok","timestamp":1620897702576,"user_tz":-540,"elapsed":19554,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # 쿼리 은닉 상태(query hidden state)는 (batch_size, hidden size)쌍으로 이루어져 있습니다.\n","    # query_with_time_axis은 (batch_size, 1, hidden size)쌍으로 이루어져 있습니다.\n","    # values는 (batch_size, max_len, hidden size)쌍으로 이루어져 있습니다.\n","    # 스코어(score)계산을 위해 덧셈을 수행하고자 시간 축을 확장하여 아래의 과정을 수행합니다.\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다.\n","    # score를 self.V에 적용하기 때문에 마지막 축에 1을 얻습니다.\n","    # self.V에 적용하기 전에 텐서는 (batch_size, max_length, units)쌍으로 이루어져 있습니다.\n","    score = self.V(tf.nn.tanh(\n","        self.W1(query_with_time_axis) + self.W2(values)))\n","\n","    # attention_weights는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다. \n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # 덧셈이후 컨텍스트 벡터(context_vector)는 (batch_size, hidden_size)쌍으로 이루어져 있습니다.\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"weUzeqB1FaVk","executionInfo":{"status":"ok","timestamp":1620897703582,"user_tz":-540,"elapsed":20559,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # 어텐션을 사용합니다.\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output는 (batch_size, max_length, hidden_size)쌍으로 이루어져 있습니다.\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # 임베딩층을 통과한 후 x는 (batch_size, 1, embedding_dim)쌍으로 이루어져 있습니다.\n","    x = self.embedding(x)\n","\n","    # 컨텍스트 벡터와 임베딩 결과를 결합한 이후 x의 형태는 (batch_size, 1, embedding_dim + hidden_size)쌍으로 이루어져 있습니다.\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # 위에서 결합된 벡터를 GRU에 전달합니다.\n","    output, state = self.gru(x)\n","\n","    # output은 (batch_size * 1, hidden_size)쌍으로 이루어져 있습니다.\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output은 (batch_size, vocab)쌍으로 이루어져 있습니다.\n","    x = self.fc(output)\n","\n","    # return x, state, attention_weights\n","    return x, state"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoeGR2CsFk8E","executionInfo":{"status":"ok","timestamp":1620897703583,"user_tz":-540,"elapsed":20558,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","decoder2 = Decoder(vocab_tar_size2, embedding_dim, units, BATCH_SIZE)\n","decoder3 = Decoder(vocab_tar_size3, embedding_dim, units, BATCH_SIZE)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFs5xbUXFmPH","executionInfo":{"status":"ok","timestamp":1620897703584,"user_tz":-540,"elapsed":20558,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7GWRTtRFoGz","executionInfo":{"status":"ok","timestamp":1620897703584,"user_tz":-540,"elapsed":20556,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["# 여기서 학습한 매개변수를 저장, optimizer/encoder/decoder\n","checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/training_checkpoints'\n","checkpoint_dir2 = '/content/drive/MyDrive/Colab Notebooks/training_checkpoints_2'\n","checkpoint_dir3 = '/content/drive/MyDrive/Colab Notebooks/training_checkpoints_3'\n","\n","# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","checkpoint2 = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder2,\n","                                 decoder=decoder2)\n","checkpoint3 = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder3,\n","                                 decoder=decoder3)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-wAUSiGFujZ","executionInfo":{"status":"ok","timestamp":1620897703585,"user_tz":-540,"elapsed":20556,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["def evaluate(sentence):\n","  # 어텐션 그래프\n","  # attention_plot = np.zeros((max_length_targ, max_length_inp))\n","\n","  sentence = preprocess_sentence(sentence)\n","\n","  # 문장, input 딕셔너리 출력 \n","  print ('sentence:', sentence)\n","\n","  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","  inputs2 = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","  inputs2 = tf.keras.preprocessing.sequence.pad_sequences([inputs2],\n","                                                         maxlen=max_length_inp2,\n","                                                         padding='post')\n","  inputs3 = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","  inputs3 = tf.keras.preprocessing.sequence.pad_sequences([inputs3],\n","                                                         maxlen=max_length_inp3,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","  inputs2 = tf.convert_to_tensor(inputs2)\n","  inputs3 = tf.convert_to_tensor(inputs3)\n","\n","\n","  result = ''\n","  result2 = ''\n","  result3 = ''\n","\n","  hidden = [tf.zeros((1, units))]\n","  hidden2 = [tf.zeros((1, units))]\n","  hidden3 = [tf.zeros((1, units))]\n","\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","  enc_out2, enc_hidden2 = encoder2(inputs2, hidden2)\n","  enc_out3, enc_hidden3 = encoder3(inputs3, hidden3)\n","\n","\n","  dec_hidden = enc_hidden\n","  dec_hidden2 = enc_hidden2\n","  dec_hidden3 = enc_hidden3\n","\n","\n","  # Target 딕셔너리 출력\n","  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","  for t in range(max(max_length_inp, max_length_inp2, max_length_targ3)):\n","    predictions, dec_hidden = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","    predictions2, dec_hidden2 = decoder(dec_input,\n","                                                         dec_hidden2,\n","                                                         enc_out2)\n","    predictions3, dec_hidden3 = decoder(dec_input,\n","                                                         dec_hidden3,\n","                                                         enc_out3)\n","    \n","    # Ensemble - Soft Voting (다른 데이터를 가지고 했을 때)\n","    predictions_sum = tf.add(predictions[0], predictions2[0], predictions3[0])\n","    mean = tf.divide(predictions_sum, 3)\n","    voting_id = tf.argmax(mean).numpy()\n","\n","    # tensorflow 에서 .numpy(): Tensor -> Numpy 로 바꿔주는 역할\n","    # predictions[0]  = tf.Tensor([-7.6920695 -7.9410057  3.7609155 ... ])\n","    \n","    result += targ_lang.index_word[voting_id] + ' '\n","    print('result: ', result)\n","\n","    if targ_lang.index_word[voting_id] == '<end>':\n","      # return result, sentence, attention_plot\n","      return result, sentence\n","\n","    # 예측된 ID를 모델에 다시 피드합니다.\n","    dec_input = tf.expand_dims([voting_id], 0)\n","    print(\"for 문 후 dec_input : \", dec_input)\n","\n","  # for t in range(max_length_targ):\n","  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n","  #                                                        dec_hidden,\n","  #                                                        enc_out)\n","\n","    # 나중에 어텐션 가중치를 시각화하기 위해 어텐션 가중치를 저장합니다.\n","    # attention_weights = tf.reshape(attention_weights, (-1, ))\n","    # attention_plot[t] = attention_weights.numpy()\n","\n","#     predicted_id_list = []   # 제일 큰 확률 5개의 id 저장 리스트 (확률 값이 아니라 해당 아이디!!!!)\n","#     search_max_predictions = list(predictions[0])  # 큰 값 5개를 찾기 위해서 임시로 복사한 확률 분포 리스트\n","#     for i in range(5):\n","#         argmax_id = tf.argmax(search_max_predictions).numpy() # 확률 제일 큰 값 \n","#         predicted_id_list.append(argmax_id)\n","#         del search_max_predictions[argmax_id]  # 찾은 제일 큰 값 리스트에서 삭제 -> 다음 루프에서 그 다음으로 큰 값 찾기\n","    \n","#     # 예측된 5개의 id 에 해당하는 단어 출력\n","#     for i in predicted_id_list:\n","#         print(i, ' : ', targ_lang.index_word[i])\n","        \n","#     # 다음 나올 단어 입력\n","#     predicted_id = int(input(\"다음 단어의 ID 를 입력하세요 : \"))\n","# #     predicted_id = max(predicted_id_list)\n","    \n","#     print('predicted_id_list : ', predicted_id_list)\n","    \n","\n","    # result += targ_lang.index_word[predicted_id] + ' '\n","    # print('result: ', result)\n","\n","    # if targ_lang.index_word[predicted_id] == '<end>':\n","    #   return result, sentence, attention_plot\n","\n","    # # 예측된 ID를 모델에 다시 피드합니다.\n","    # dec_input = tf.expand_dims([predicted_id], 0)\n","    # print(\"for 문 후 dec_input : \", dec_input)\n","\n","  # return result, sentence, attention_plot\n","  return result, sentence"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1uJo_rwvQxJ","executionInfo":{"status":"ok","timestamp":1620897703585,"user_tz":-540,"elapsed":20554,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}}},"source":["def translate(sentence):\n","  result, sentence = evaluate(sentence)\n","\n","  print('Input: %s' % (sentence))\n","  print('Predicted translation: {}'.format(result))\n","\n","  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","#   plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypD8jH0QvV6-","executionInfo":{"status":"ok","timestamp":1620897711522,"user_tz":-540,"elapsed":28485,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"efd7556d-6bd9-44b9-d0fd-ac54e5afa0b3"},"source":["# checkpoint_dir내에 있는 최근 체크포인트(checkpoint)를 복원\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_dir2))\n","checkpoint3.restore(tf.train.latest_checkpoint(checkpoint_dir3))"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb167f8a890>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"cUDjZTCKvZA5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620897842607,"user_tz":-540,"elapsed":1096,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"590facb8-29e8-40ae-d63e-ae4ff55bcb6c"},"source":["translate(u'hace mucho frio aqui.')  # it s very cold here"],"execution_count":28,"outputs":[{"output_type":"stream","text":["sentence: <start> hace mucho frio aqui . <end>\n","result:  it \n","for 문 후 dec_input :  tf.Tensor([[10]], shape=(1, 1), dtype=int32)\n","result:  it s \n","for 문 후 dec_input :  tf.Tensor([[11]], shape=(1, 1), dtype=int32)\n","result:  it s very \n","for 문 후 dec_input :  tf.Tensor([[48]], shape=(1, 1), dtype=int32)\n","result:  it s very cold \n","for 문 후 dec_input :  tf.Tensor([[183]], shape=(1, 1), dtype=int32)\n","result:  it s very cold is \n","for 문 후 dec_input :  tf.Tensor([[8]], shape=(1, 1), dtype=int32)\n","result:  it s very cold is good \n","for 문 후 dec_input :  tf.Tensor([[69]], shape=(1, 1), dtype=int32)\n","result:  it s very cold is good early \n","for 문 후 dec_input :  tf.Tensor([[287]], shape=(1, 1), dtype=int32)\n","result:  it s very cold is good early . \n","for 문 후 dec_input :  tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n","result:  it s very cold is good early . <end> \n","Input: <start> hace mucho frio aqui . <end>\n","Predicted translation: it s very cold is good early . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"76Goju9Rvch8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620897782457,"user_tz":-540,"elapsed":99413,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"308fa4b7-0ef0-41de-e3be-565b14411f83"},"source":["translate(u'esta es mi vida.')  # this is my life"],"execution_count":23,"outputs":[{"output_type":"stream","text":["sentence: <start> esta es mi vida . <end>\n","result:  this \n","for 문 후 dec_input :  tf.Tensor([[19]], shape=(1, 1), dtype=int32)\n","result:  this is \n","for 문 후 dec_input :  tf.Tensor([[8]], shape=(1, 1), dtype=int32)\n","result:  this is my \n","for 문 후 dec_input :  tf.Tensor([[21]], shape=(1, 1), dtype=int32)\n","result:  this is my life \n","for 문 후 dec_input :  tf.Tensor([[189]], shape=(1, 1), dtype=int32)\n","result:  this is my life . \n","for 문 후 dec_input :  tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n","result:  this is my life . <end> \n","Input: <start> esta es mi vida . <end>\n","Predicted translation: this is my life . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dvrhSTomviTt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620897831622,"user_tz":-540,"elapsed":949,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"c7902bce-2a94-4db8-af60-4667e6121cef"},"source":["translate(u'¿todavia estan en casa?')  # Are you still at home?"],"execution_count":27,"outputs":[{"output_type":"stream","text":["sentence: <start> ¿ todavia estan en casa ? <end>\n","result:  are \n","for 문 후 dec_input :  tf.Tensor([[24]], shape=(1, 1), dtype=int32)\n","result:  are you \n","for 문 후 dec_input :  tf.Tensor([[6]], shape=(1, 1), dtype=int32)\n","result:  are you still \n","for 문 후 dec_input :  tf.Tensor([[103]], shape=(1, 1), dtype=int32)\n","result:  are you still home \n","for 문 후 dec_input :  tf.Tensor([[89]], shape=(1, 1), dtype=int32)\n","result:  are you still home ? \n","for 문 후 dec_input :  tf.Tensor([[7]], shape=(1, 1), dtype=int32)\n","result:  are you still home ? <end> \n","Input: <start> ¿ todavia estan en casa ? <end>\n","Predicted translation: are you still home ? <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OTXXZwkdvg_Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620899419560,"user_tz":-540,"elapsed":1005,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"12d7e1c6-efec-49df-c926-01a6209f0bb9"},"source":["# 잘못된 번역\n","translate(u'trata de averiguarlo.')   # try to find out / try to figure out"],"execution_count":29,"outputs":[{"output_type":"stream","text":["sentence: <start> trata de averiguarlo . <end>\n","result:  try \n","for 문 후 dec_input :  tf.Tensor([[125]], shape=(1, 1), dtype=int32)\n","result:  try to \n","for 문 후 dec_input :  tf.Tensor([[15]], shape=(1, 1), dtype=int32)\n","result:  try to figure \n","for 문 후 dec_input :  tf.Tensor([[891]], shape=(1, 1), dtype=int32)\n","result:  try to figure it \n","for 문 후 dec_input :  tf.Tensor([[10]], shape=(1, 1), dtype=int32)\n","result:  try to figure it out \n","for 문 후 dec_input :  tf.Tensor([[74]], shape=(1, 1), dtype=int32)\n","result:  try to figure it out of \n","for 문 후 dec_input :  tf.Tensor([[59]], shape=(1, 1), dtype=int32)\n","result:  try to figure it out of us \n","for 문 후 dec_input :  tf.Tensor([[81]], shape=(1, 1), dtype=int32)\n","result:  try to figure it out of us . \n","for 문 후 dec_input :  tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n","result:  try to figure it out of us . <end> \n","Input: <start> trata de averiguarlo . <end>\n","Predicted translation: try to figure it out of us . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwmBqLU8cvE6","executionInfo":{"status":"ok","timestamp":1620897782460,"user_tz":-540,"elapsed":99409,"user":{"displayName":"박아정","photoUrl":"","userId":"15223679537180029561"}},"outputId":"1655660a-779d-4395-c45c-8811bf2f66d0"},"source":["translate(u'Te quiero')   # I love you"],"execution_count":26,"outputs":[{"output_type":"stream","text":["sentence: <start> te quiero <end>\n","result:  i \n","for 문 후 dec_input :  tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n","result:  i m \n","for 문 후 dec_input :  tf.Tensor([[18]], shape=(1, 1), dtype=int32)\n","result:  i m staying \n","for 문 후 dec_input :  tf.Tensor([[1383]], shape=(1, 1), dtype=int32)\n","result:  i m staying . \n","for 문 후 dec_input :  tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n","result:  i m staying . <end> \n","Input: <start> te quiero <end>\n","Predicted translation: i m staying . <end> \n"],"name":"stdout"}]}]}